---
title: "Daily arXiv Scan: February 27, 2026"
date: 2026-02-27
tags: ["arxiv", "ai-research", "frontier-ai", "safety", "agents", "reasoning", "efficiency"]
categories: ["Frontier AI Research"]
---

# Frontier AI Research Scan ‚Äî February 27, 2026

Safety datasets that don't measure safety, agents that score well but fail unpredictably, and the discovery that LLMs have a hard ceiling on how far they can reason. Plus: the International AI Safety Report drops, and a new way to make reasoning cheaper without making it worse.

---

## üß® Intent Laundering: AI Safety Datasets Are Not What They Seem
**[arXiv:2602.16729](https://arxiv.org/abs/2602.16729)** ‚Äî Golchin et al.

The most important safety paper this week. The authors show that widely used AI safety benchmarks don't actually measure resistance to adversarial attacks ‚Äî they measure resistance to *obvious trigger words*. The technique: "intent laundering," which strips away overt negative language while preserving the full malicious intent. Result? Models previously rated as "reasonably safe" ‚Äî including Gemini 3 Pro and Claude Sonnet 3.7 ‚Äî become unsafe. Black-box attack success rates hit 90-98%.

**Quick take:** This should be a five-alarm fire for every safety team. If your eval only catches attacks that *look* dangerous, you're testing for pattern matching, not understanding. The name "intent laundering" is perfect ‚Äî it's exactly what sophisticated adversaries already do. Every safety benchmark needs to be re-examined through this lens.

---

## üìä Towards a Science of AI Agent Reliability
**[arXiv:2602.16666](https://arxiv.org/abs/2602.16666)** ‚Äî Rabanser et al. (Princeton)

Benchmark accuracy keeps climbing, but agents keep failing in deployment. Why? Because a single success metric hides everything that matters: consistency across runs, robustness to perturbations, predictability of failure modes, and severity of errors. This paper proposes 12 metrics across four dimensions (consistency, robustness, predictability, safety) and evaluates 14 models. The finding: 18 months of capability progress has produced only marginal reliability gains.

**Quick take:** This is the paper to cite when someone shows you an agent leaderboard and asks "why doesn't this work in production?" Capability ‚â† reliability, and until the field internalizes this, deployed agents will keep surprising people in bad ways. The [interactive dashboard](https://hal.cs.princeton.edu/reliability) is worth exploring.

---

## üß† Limited Reasoning Space: The Cage of Long-Horizon Reasoning in LLMs
**[arXiv:2602.19281](https://arxiv.org/abs/2602.19281)**

A theoretical result with practical teeth. The authors formalize the "Limited Reasoning Space" hypothesis: for any given prompt, there's an intrinsic upper bound on how far an LLM can effectively reason. Push past it with more chain-of-thought steps and performance *collapses* ‚Äî redundant feedback accumulates and actively impairs the model. Their fix, Halo, uses model predictive control to dynamically regulate planning depth at the reasoning boundary.

**Quick take:** This explains something practitioners already feel ‚Äî that longer CoT doesn't always help and sometimes actively hurts. The framing as a dynamical system with bounded horizons is elegant. If you're scaling test-time compute, you need to read this before throwing more tokens at the problem.

---

## ‚ö° IAPO: Information-Aware Policy Optimization for Token-Efficient Reasoning
**[arXiv:2602.19049](https://arxiv.org/abs/2602.19049)** ‚Äî He et al.

Reasoning models burn tokens. IAPO fights back with information theory: each token gets scored by its mutual information with the final answer, and RL training uses these scores to suppress low-utility reasoning steps. The result: up to 36% fewer reasoning tokens with no accuracy loss ‚Äî and in many cases, accuracy *improves*. The key insight is that sequence-level reward shaping can't control *where* reasoning effort goes; you need token-level signals.

**Quick take:** This is the efficiency paper I've been waiting for. The "think harder" scaling paradigm is expensive and wasteful when half the reasoning chain is filler. IAPO's information-theoretic framing is principled and the results are strong. Expect this approach to show up in production reasoning systems quickly.

---

## üåê International AI Safety Report 2026
**[arXiv:2602.21012](https://arxiv.org/abs/2602.21012)** ‚Äî Bengio, Clare, Prunkl et al.

The second edition of the report mandated by the AI Safety Summit. Led by Yoshua Bengio with a massive author list spanning governments, industry, and academia. Synthesizes current scientific evidence on general-purpose AI capabilities, emerging risks, and safety. This isn't a research paper ‚Äî it's the closest thing we have to a consensus document on where AI safety stands in 2026.

**Quick take:** Read this as a state-of-the-field snapshot for policymakers. The fact that it exists on arXiv (not just as a government PDF) means it'll get more scrutiny from the research community. The gap between what this report identifies as risks and what the Intent Laundering paper above demonstrates as *current vulnerabilities* is... instructive.

---

## üîß Multi-Layer Scheduling for MoE-Based LLM Reasoning
**[arXiv:2602.21626](https://arxiv.org/abs/2602.21626)** ‚Äî Sun et al.

Systems paper. MoE models are everywhere now, but serving them efficiently means solving scheduling at three levels simultaneously: request routing, engine dispatch, and expert-level load balancing. This framework tackles all three with load-aware strategies that account for KV cache state, prefix sharing, and expert hotspots. 17.8% TTFT reduction over vLLM across 100+ experiments.

**Quick take:** Not glamorous, but this is the infrastructure work that actually determines whether frontier models are usable in practice. The expert-level scheduling ‚Äî dealing with routing imbalance and inter-layer dependencies ‚Äî is a problem unique to MoE that most serving frameworks still handle poorly. Nearly 18% latency reduction is significant at scale.

---

*Theme of the day: The gap between benchmark performance and real-world reliability keeps widening. Safety evals don't measure real attacks. Agent benchmarks don't predict deployment success. Reasoning chains have hard ceilings. The field is getting better at naming these problems ‚Äî now it needs to fix them.*
